{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting Started with Neural Networks\n",
    "\n",
    "# Contents:\n",
    "#     1) Core components of neural networks\n",
    "#     2) An introduction to keras\n",
    "#     3) Setting up a deep-learning workstation\n",
    "#     4) Using neural networks to solve basic classification and regression problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Anatomy of a Neural Network\n",
    "\n",
    "# 1) Layers, which are combined into a network\n",
    "# 2) The input data and corresponding targets\n",
    "# 3) Loss function, which defines the feedback signal used for learning\n",
    "# 4) The optimizer, which determines how learning proceeds\n",
    "\n",
    "\n",
    "# Steps: \n",
    "#     First input x is inputted into the first layer, and random weights are assigned.\n",
    "#     Next, the input is passed into another layer, and predictions are received. The \n",
    "#     actual values are then bundled with the predictions and passed through the loss \n",
    "#     function, which then computes a loss score. The loss score is then passed into\n",
    "#     the optimizer, which then updates the weights for the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layers: the building blocks of deep learning\n",
    "\n",
    "# More layers have a state: the layer's weights, one or several tensors learned with stochastic \n",
    "# gradient descent, which together contain the network's knowledge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Models: Networks of Layers\n",
    "    \n",
    "# A deep learning model is a directed, acyclic graph of layers. The most common instance is a \n",
    "# linear stack of layers, mapping a single input to a single output.\n",
    "\n",
    "# There are a lot of different network topologies/layouts\n",
    "\n",
    "# The topology of a network defines a hypothesis space. By choosing a specific network topology,\n",
    "# you constrain your hyopthesis space to a specific series of tensor operations, mapping input\n",
    "# data to output data. This lets you search for a good set of values for the weight tensors involved\n",
    "# in these tensor operations\n",
    "\n",
    "# Choose the right network architecture for the right problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss Functions and Optimizers: Keys to Configuring the Learning Process\n",
    "    \n",
    "# Once the network architecture is defined, need to choose:\n",
    "#     1) Loss function: the quantity that will be minimized during training. Represents a measure of\n",
    "#        success\n",
    "#     2) Optimizer: Determines how the network will be updated based on the loss function. Implrements\n",
    "#        a specific variant of stochastic gradeient descent (SGD)\n",
    "    \n",
    "# A neural network that has multiple outputs may have multiple loss functions (one per output), but the\n",
    "# gradient-descent process must be based on a single scalar loss value, so for multiloss networks, all\n",
    "# losses are combined into a single scalar quantity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Different Sets of Data\n",
    "\n",
    "# Training set: A set of examples used for learning: to fit the parameters of the classifier \n",
    "#               In the MLP case, we would use the training set to find the “optimal” weights \n",
    "#             with the back-prop rule\n",
    "\n",
    "# Validation set: A set of examples used to tune the parameters of a classifier In the MLP \n",
    "#                 case, we would use the validation set to find the “optimal” number of hidden \n",
    "#                 units or determine a stopping point for the back-propagation algorithm\n",
    "\n",
    "# Test set: a set of examples used only to assess the performance of a fully-trained classifier \n",
    "#           In the MLP case, we would use the test to estimate the error rate after we have chosen \n",
    "#           the final model (MLP size and actual weights) After assessing the final model on the test \n",
    "#           set, YOU MUST NOT tune the model any further!\n",
    "\n",
    "# Why separate test and validation sets? The error rate estimate of the final model on validation data \n",
    "# will be biased (smaller than the true error rate) since the validation set is used to select the final \n",
    "# model After assessing the final model on the test set, YOU MUST NOT tune the model any further!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
