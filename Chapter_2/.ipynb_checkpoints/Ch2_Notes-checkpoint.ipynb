{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Mathematical Building Blocks of Neural Networks\n",
    "\n",
    "# Contents:  \n",
    "#     1) First example of a neural network using MNIST sample digitis\n",
    "#     2) Tensors and tensor operations\n",
    "#     3) How neuarl networks learn via backpropogation and gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Representations for Neural Networks: \n",
    "\n",
    "#     1) Scalars: 0-D tensors, only one number\n",
    "\n",
    "#     2) Vectors: 1-D tensors\n",
    "    \n",
    "#     3) Matrices: 2-D tensors\n",
    "    \n",
    "#     4) 3D Tensors/higher dimensions\n",
    "\n",
    "# Key Attributes: \n",
    "#     1) number of axes: rank\n",
    "#     2) shape: tuple of integres that describes how many dimensions the tensor has along each axis\n",
    "#     3) Data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient-based optimization\n",
    "\n",
    "# output = relu(dot(W, input) + b)\n",
    "# Where W and b are tensors that are attributes of the layer (weights)\n",
    "\n",
    "\n",
    "# Training Loop:\n",
    "#     1) Draw a batch of training samples x and corresponding targets y\n",
    "#     2) Run the network on x to obtain predictions y_pred\n",
    "#     3) Compute the loss of the network on the batch, a measure of the mismatch between y_pred and y\n",
    "#     4) Update all the weights of the network to slightly reduce the loss on this batch\n",
    "\n",
    "# If you continue the training loop, you'll eventually end up with a network that has a very low loss\n",
    "# on its training data\n",
    "\n",
    "# Mini-batch stochastic gradient descent:\n",
    "#     1) Draw a batch of training samples and corresponding targets y\n",
    "#     2) Run the network on x to obtain predictions y_pred\n",
    "#     3) Compute the loss of the network on the batch, a measure of the mismatch between y_pred and y\n",
    "#     4) ompute the gradient of the loss with regard to the network's parameters (a backward pass)\n",
    "#     5) Move the parameters a little in the opposite direction from the gradient, reducing the loss\n",
    "#        on the batch a bit.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
