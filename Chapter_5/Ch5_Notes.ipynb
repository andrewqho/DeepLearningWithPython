{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep Learning For Computer Vision\n",
    "\n",
    "# Contents:\n",
    "#     1) Understanding convolutional neural networks\n",
    "#     2) Using data augmentation to mitigate overfitting\n",
    "#     3) Using a pretrained convnet to do feature extraction\n",
    "#     4) Fine-tuning a pretrained convnet\n",
    "#     5) Visualizing what convnets learn and how they make classification decisions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Convolutional Operation\n",
    "\n",
    "# The fundamental difference between a densely connected layer and a convolution layer is that Dense layers learn global\n",
    "# patterns in their input feature space, whereas convolution layers learn local patterns. In the case of images, \n",
    "# patterns found in small 2d windows of the input. In the MNIST example, the windows were all 3x3.\n",
    "\n",
    "# Convnets thus have two interesting properties:\n",
    "#     1) The patterns they learn are translation invariant. This means after learning a certain pattern in one area\n",
    "#        of a picture, the convnet can recognize it anywhere. A densely connected network would have to learn the \n",
    "#        pattern anew if it appeared at a new location. This makes convnets data efficient when processing images, \n",
    "#        because they need fewer training samples to learn represenations that have gneralization power \n",
    "#     2) The can learn spatial hierarchies of patterns. A first convolution layer will learn small local patterns such\n",
    "#        as edges, a second convolution layer will learn larger patterns made of the features of the first layers, and\n",
    "#        so on. This allows convnets to efficiently learn increasingly complex and abstract visual concepts.\n",
    "    \n",
    "#        For example, the first layer can learn edges, the second layer can learn how to put the edges together into\n",
    "#        an ear or eyes, then the last layer can build, from the features of the second layer, a cat or some shit.\n",
    "    \n",
    "# Convolutions operate over 3D tensors, called feature maps, with two spatial axes (width and height), as well as a \n",
    "# depth axis, called a channel axis. For an RGB image, the dimension of the depth is 3, because the image has three \n",
    "# color channels: red, green, and blue. For a black and white picture, the depth is 1 (levels of gray). The convolution\n",
    "# operation extracts patches from its input feature map and applies the same transformation to all of these patches, \n",
    "# producing an output feature map. This output feature map is still a 3D tensor: it has a width and a height. Its depth \n",
    "# can be arbitrary, because the output depth is a parameter of the layer, and different channels in that depth axis no \n",
    "# longer stand for specific colors as in RGB input; rather they stand for filters    \n",
    "    \n",
    "# In the MNIST example, the first convolution layer takes a feature map of size (28, 28, 1) and outputs a feature map \n",
    "# of size (26, 26, 32): it computes 32 filters over its input. Each of these 32 output channels contain a 26x26 grid \n",
    "# of values, which is a response map of the filter over the input, indicating the response of that filter pattern at \n",
    "# different locations in the input. \n",
    "\n",
    "# Convolutions are defined by two key parameters:\n",
    "#     1) Size of the patches extracted from the inputs\n",
    "#     2) Depth of the output feature map\n",
    "\n",
    "# A convolution works by sliding these windows of size 3x3 or 5x5 over the 3D input feature map, stopping at every \n",
    "# possible location, and extracting the 3D patch of surrounding features, such as window_height, window_width, and \n",
    "# input_depth. Each such 3D patch is then transformed via a tensor product with the same learned weight matrix, called \n",
    "# the convolution kernel, into a 1D vector of shape (output_depth). These vectors are then spatially reassembled into \n",
    "# a 3D output map of shape (height, width, output_depth). Every spatial location in the output feature map corresponds \n",
    "# to the same location in the input feature map. For example, the lower-right corner of the output containes information\n",
    "# about the lower right corner of the input. For instance, the 3x3 windows, the vector output [i-1:i+1, j-1:j+1, :] \n",
    "# comes from the 3D patch input[i-1:i+1, j-1:j+1, :]\n",
    "    \n",
    "# The output width and height may differ from the input width and height. They may differ for two reasons:\n",
    "#     1) border effects, which can be countered by padding the input feature map\n",
    "#     2) The use of strides\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Border Effects and Padding\n",
    "\n",
    "# Consider a 5x5 feature map, for 25 tiles total. There are only 9 tiles around which you could center a 3x3 window, \n",
    "# forming a 3x3 grid. Hence the output feature map will be 3x3. It shrinks a little, by exactly two tiles alongside \n",
    "# each dimension. You can see this border effect in action in the earlier example, where we start with 28x28 inputs \n",
    "# and get back a 26x26 after the first convolution layer.\n",
    "\n",
    "# Padding:\n",
    "#     If you want to get an output feature map with the same spatial dimensions as the input, you can use padding. \n",
    "#     Padding consists of adding an appropriate number of rows and columns on each side of the input feature map \n",
    "#     so as to make it possible to fit center convolution windows around every input tile. \n",
    "    \n",
    "#     For a 3x3 window, you add one column to every side. \n",
    "#     For a 5x5, you add two columns to every side\n",
    "    \n",
    "#     In Conv2D layers, ladding is confiurable via the padding argument, which takes two values: \n",
    "#         \"valid\", which means no padding, and only take window loacations that are valid\n",
    "#         \"same\", which means pad in such a way as to have an output with the same width and height as the input\n",
    "        \n",
    "        \n",
    "    \n",
    "# Convolution strides:\n",
    "#     The other factor that can influence output size is the notion of \"strides\". \n",
    "#     The description of convolution so far has assumed that the center tiles of the convolution windows are \n",
    "#     all contiguous.\n",
    "#     However, the distance bewteen two successive windows is a parameter of the convolution, called its stride, \n",
    "#     which defaults to 1. It's possible to have strided convolutions, which are convolutions with a stride higher\n",
    "#     than 1. \n",
    "    \n",
    "#     Using stride 2 means the width and height of the feature map are downsampled by a factor of 2. \n",
    "#     Strided convolutions are rarely used in practice, although they come in handy for some types of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-95d557437a91>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-95d557437a91>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Max Pooling\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Max Pooling \n",
    "\n",
    "# The role of max pooling is to aggressively downsample feature maps, much like strided convolutions\n",
    "\n",
    "# Max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel.\n",
    "\n",
    "# It's like convolution, but instead of transforming the local patches, they are transformed via a hardcoded max tensor\n",
    "# operation. Max pooling is usually done in 2x2 windows in order to downsample the feature maps by a factor of 2.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
