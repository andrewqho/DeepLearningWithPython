{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fundamentals of Machine Learning\n",
    "\n",
    "# Contents:\n",
    "#     1) Forms of machine learning beyond classification and regression\n",
    "#     2) Formal evaluation procedures for machine-learning\n",
    "#     3) Formal evaluation procedures for machine-learning models\n",
    "#     4) Preparing data for deep learning\n",
    "#     5) Feature engineering\n",
    "#     6) Tackling overfitting\n",
    "#     7) The universal workflow for approaching machine learning problems\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Four Branches of Machine Learning\n",
    "\n",
    "# 1) Supervised Learning\n",
    "#    Supervised learning is the most common case. It consists of learning to map input data to known targets, known\n",
    "#    as annotations given a set of examples, often times annotated by humans. Generally, almost all applications of \n",
    "#    deep learning that are in the spotlight these days belong in this category, such as optical characer recognition, \n",
    "#    speech recognition, speech recognition, image classification, and language translation.\n",
    "\n",
    "#    Most supervised learning mostly consists of classification and regression, there are more exotic variants as well:\n",
    "#     - Sequence generation: Given a picture, predict a caption describing it\n",
    "#     - Syntax tree prediction: Given a sentence, predict its decomposition into a syntax tree\n",
    "#     - Object detection: Given a picture, draw a bounding box around certain objects inside the picture. This can\n",
    "#       also be expressed as a classification problem, or a joint classification and regression problem\n",
    "#     - Image segmentation: Given a picture, draw a pixel-level mask on a specific object\n",
    "\n",
    "# 2) Unsupervised Learning\n",
    "#    This branch of machine learning consists of finding interesting transformations of the input data without the help \n",
    "#    of any targets, tfor the purposes of data visualization, data compression, or data denoising, or to better \n",
    "#    understand the correlatoins present in the data at hand. Unsupervised learning is the bread and butter of data \n",
    "#    analytics, and it's often a necessary step in better understanding a dataset before attempting to solve a \n",
    "#    supervised-learning problem. Dimensionality reduction and clustering are well-known categories of unsupervised \n",
    "#    learning. \n",
    "\n",
    "# 3) Self-supervised learning\n",
    "#    This is a specific instance of supervised learning, but it's different enough to be a different category. This is\n",
    "#    essentially supervised learning without any humans in the loop. There are still labels involved, because the \n",
    "#    learning has to be supervised by something, but they're generated from the input data, typically using a heuristic \n",
    "#    algorithm.  \n",
    " \n",
    "# 4) Reinforcement learning\n",
    "#    In reinforcement learning, an agent receives information about its environment and learns to choose actions that \n",
    "#    will maximize some reward. For instance, a neuralnetwork that looks at a video game screen and outputs game \n",
    "#    actions in order to maximize its score can be trained via reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classification and Regression Glossary:\n",
    "    \n",
    "# Sample/input - One data popint that goes into your model\n",
    "# Prediction/output - What comes out of th model\n",
    "# Target - The truth, or what your model should ideally have predicted, according to an external source of data\n",
    "# Prediction error/loss value - A measure of the distance between your model's prediction and the target\n",
    "# Classes - A set of possible labels to choose from in a classification problem\n",
    "# Label - A specific instance of a class annotation in a classification problem\n",
    "# Annotations - All targets for a dataset, typically collected by humans\n",
    "# Binary classification - A classification task where each input sample should be categorized into two exclusive \n",
    "#                         categories\n",
    "# Multiclass classification - A classification task where each input sample should be categorized into two exclusive \n",
    "#                             categories.\n",
    "# Scalar regression - A task where the target is a continuous scalar value\n",
    "# Vector regression - A task where the target is a set of continuous values\n",
    "# Mini-batch/batch - A small set of samples that are processed simultaneously by the model. Usually the number of \n",
    "#                    samples is often a power of 2, to facilitate memory allocation on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating machine-learning models\n",
    "\n",
    "# Developing a model always involves tuning its confiruation, like choosing the number of layers or the size of the \n",
    "# layers (called hyper-parameters of the model, to distinguish them from the parameters, which are the network's \n",
    "# weights). \n",
    "\n",
    "# Tuning is a form of learning, a search for a good configuration in some parameter space. As a result, tuning the \n",
    "# configuration of the model based on its performance on the validation set can quickly result in overfitting to \n",
    "# the validation set, even though your model is never directly trained on it.\n",
    "\n",
    "# Every time you tune a hyperparameter of your model based on the model's performance on the validation set, some \n",
    "# information about the validation data leaks into the model. If you do this only once, for one parameter, then\n",
    "# very few bits of information will leak, and your validation set will remain reliable to evaluate the model. But\n",
    "# if you repeat this many times - running one experiment, evaluating on the validation set, and modifying your model \n",
    "# as a result - then you'll leak an increasingly significant amount of information about the validation set into \n",
    "# the model.\n",
    "\n",
    "# There are some different ways to set apart data:\n",
    "#     1) Simple hold-out validation: Set apart some fraction of your data as your test set. Train on the remaining\n",
    "#        data, and evaluate on the test set. \n",
    "#     2) K-Fold Validation: Split data into K partitions of equal size. For each partition i, train a model on the \n",
    "#        remaining K-1 partitions, and evaluate it on partition i. Your final score is then the averages of the K \n",
    "#        scores obtained. This method is helpful when the performance of your model shows significant variance based \n",
    "#        on your train-test split. Like hold-out validation, this method does not exempt you from using a distinct \n",
    "#        validation set for model calibration.\n",
    "#     3) Iterated K-Fold Validation with shuffling: This is for when you have relatively little data available and you \n",
    "#        need to evaluate your model as precisely as possible. It applies K-fold validation multiple times, shuffling \n",
    "#        the data every time before splitting it K-ways. The final score is the average of the scores obtained at each \n",
    "#        run of K-fold validation. Note that you end up training and evaluating P*K models where P is the number of \n",
    "#        iterations you use, which can be very expensive.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing, Feature Engineering, and Feature Learning\n",
    "\n",
    "# Vectorization:\n",
    "#     The process of turning all inputs and targets in a neural network to tensors of floating-point data\n",
    "#     or in some cases, tensors of integers.\n",
    "\n",
    "# Value Normalization:\n",
    "#     We have done value normalization twice already. In the digit classification example, we started from image data \n",
    "#     encoded as integers in the 0-255 range, encoding grayscale values, and casted the dated to float32 and divided by\n",
    "#     255 in order to get floating-point values in the 0-1 range. In the house prices example, we started from features \n",
    "#     that took a variet of ranges - some already floating point values and some integers - and we normalized each \n",
    "#     feature independently so that it had a standard deviation of 1 and a mean of 0.\n",
    "\n",
    "#     To make learning easier for the network, we should:\n",
    "#         1) Take small values (0-1 range typically)\n",
    "#         2) Be homogenous, so all features should take values roughly in the same range\n",
    "#         3) Where you can, normalize each feature independently to have a mean of 0 and a standard deviation of 1\n",
    "        \n",
    "# Feature Engineering:\n",
    "#     Feauture Engineering is the process of using your own knowledge about the data and about the machine-learning \n",
    "#     algorithm at hand (in this case, a neural network), to make the algorithm work better by applying hardcoded \n",
    "#     transformations to the data before it goes into the model. \n",
    "    \n",
    "#     Example of feature engineering:\n",
    "#         Suppose we want to create a model that inputs an image of a clock and outputs the time of day. \n",
    "        \n",
    "#         Approach 1: We can use the raw pixels of the image as input data, but then we have a big problem on our\n",
    "#                     hands. We'll need a convlutional neural netwrok to solve it, which is quite expensive.\n",
    "#         Approach 2: We can come up with a much better input feature for a machine learning algorithm. We can\n",
    "#                     write a Python script to follow the black pixels of the clock hands and output the (x, y) \n",
    "#                     coordinates of the tip of each hand. Then a simple machine learning algorithm can learn to \n",
    "#                     associate these coordinates with the right time of day.\n",
    "                    \n",
    "#     Feature egineering is important because:\n",
    "#         1) Good features allow you to solve problems more elegantly while using fewer resources\n",
    "#         2) Good features let you solve a problem with far less data. The ability of deep learning models to\n",
    "#            learn features on their own relies on having lots of training data available. If you only have a\n",
    "#            few samples, then the information value in their features becomes critical.\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Overfitting and underfitting\n",
    "\n",
    "# The fundamental issue in machine learning is the tension between optimization and generalization\n",
    "\n",
    "# Optimization is the process of adjusting a model to get the best performance possible on the training data\n",
    "# Generalization refers to how well the trained model performs on data it has never seen before\n",
    "\n",
    "# To prevent a model from learning misleading/irrelevant patters in the training data, the best solution is\n",
    "# to get more training data. However, when you can't do this, you need to use modeulate the quantity of  information\n",
    "# that you rmodel si allowed to store, or add constraints on what information it's allowed to store. If a network \n",
    "# can only afford to memorize a small umber of patterns, the opitmization process will force it to to focus on the \n",
    "# most prominent patterns, which have a better chance of generalizing well.\n",
    "\n",
    "# Regularization is the process of fighting overfitting in a model.\n",
    "\n",
    "# Methods of Regularization:\n",
    "    \n",
    "#     Reducing the network's size:\n",
    "#         The simplest way to prevent overfitting is to reduce the number of learnable parameters in the model. The \n",
    "#         number of learnable parameters in a model is often referred to as the model's capacity. Intuitively, a model \n",
    "#         with more parameters has more memorization capacity and therefore can learn a perfect dictionary-like mapping \n",
    "#         between training samples and their targets, which is a mapping without any generalization power. At the same \n",
    "#         time, your model shouldn't be starved for memorization resources.    \n",
    "\n",
    "#     Adding weight regularization:\n",
    "#         Occam's razor states that given two explanations for something, the explanation most likely to be correct \n",
    "#         is the simplest one - the one that makes fewer assumptions. Similarly, simpler models are less likely to\n",
    "#         overfit than complex ones. \n",
    "        \n",
    "#         A simple model in this context is a model where the distribution of parameter values has less entropy, or\n",
    "#         a model with fewer parameters. \n",
    "        \n",
    "#         Weight regularization is a common way to mitigate overfitting, which works by putting  constraints on the \n",
    "#         complexity of a network by forcing its weights to only takes small values to be more regular. This is done\n",
    "#         by adding a cost associated with having large wieght values to the loss function.  \n",
    "        \n",
    "#         There are two main types of Weight regularization:\n",
    "#             1) L1 Regularization - the cost added is proportional to the absolute value of the weight coefficients\n",
    "#             2) L2 Regularization - The cost added is proportional to the square of the values of the weight \n",
    "#                coefficients\n",
    "            \n",
    "#     Adding dropout:\n",
    "#         This technique was inspired by the fraud-prevention mechanism used by banks. At banks, tellers are always \n",
    "#         changing, because cooperation between tellers would allow them to defraud the banks. By keeping them \n",
    "#         independent, it helped prevent people from coming together to make schemes. \n",
    "        \n",
    "#         Similarly, dropout prevents cooperation between units in the graph by zeroing them out. By randomly \n",
    "#         removing certain units, it becomes harder for units to create/detect arbitrary, unwanted patterns. \n",
    "        \n",
    "#         Dropout works well in practice because it prevents the co-adaption of neurons during the training phase.\n",
    "        \n",
    "#         At test time, no units are dropped out. Instead, the layer's output values are scaled up by a factor \n",
    "#         equal to the droupout rate, to balance for the fact that more units are active than at training time and\n",
    "#         that all values are inputted.\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Universal Workflow of Machine Learning\n",
    "\n",
    "Defining the problem and assembling a dataset:\n",
    "    First, you must define the problem at hand:\n",
    "        What will your input data be? What are you trying to predict?\n",
    "        What tye of problem are you facing? \n",
    "        \n",
    "        You hypothesize that your outputs can be predicted given your inputs\n",
    "        You hypothesize that your available data is sufficiently informative to\n",
    "            learn the relatioship between inputs and outputs\n",
    "\n",
    "Choosing a measure of sucess\n",
    "\n",
    "Deciding on an evaluation protocol\n",
    "\n",
    "Preparing your data\n",
    "\n",
    "Developing a model that does better than a baseline\n",
    "\n",
    "Scale up: developing a model that overfits\n",
    "    \n",
    "Regularizing your model and tuning your hyperparameters\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
